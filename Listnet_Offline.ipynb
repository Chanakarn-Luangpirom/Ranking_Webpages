{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e9ea992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('train_cleaned_small.csv')\n",
    "df_val = pd.read_csv('val_cleaned_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2042d554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26288899e30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14107655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    query_ids = sorted(list(set(x['query_id'])))\n",
    "    ys_train = np.array(x[x['query_id']==query_ids[0]]['relevance_label'].tolist())\n",
    "    ys_train_final = []\n",
    "    ys_train_final.append(ys_train)\n",
    "    for i in range(len(query_ids)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        y_new = np.array(x[x['query_id']==query_ids[i]]['relevance_label'].tolist())\n",
    "        ys_train_final.append(y_new)\n",
    "    \n",
    "\n",
    "    ys_train = torch.tensor(ys_train_final,dtype=torch.float32)\n",
    "    \n",
    "    X = np.array(x[x['query_id']==query_ids[0]].iloc[:,2:])\n",
    "    x_train_final = []\n",
    "    x_train_final.append(X)\n",
    "    for i in range(len(query_ids)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        x_new = np.array(x[x['query_id']==query_ids[i]].iloc[:,2:])\n",
    "        x_train_final.append(x_new)\n",
    "    \n",
    "\n",
    "    X_train = torch.tensor(np.array(x_train_final),dtype=torch.float32)\n",
    "    \n",
    "    return X_train,ys_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2552b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(scores):\n",
    "    scores = np.array(scores,dtype = float)\n",
    "    num = 2**scores-1\n",
    "    for i in range(len(num)):\n",
    "        num[i] /= np.log2(i+2)\n",
    "    return np.sum(num)\n",
    "\n",
    "\n",
    "def ndcg_k(scores, k):\n",
    "    top_k = scores[:k]\n",
    "    ideal_top_k = sorted(scores)[::-1][:k]\n",
    "    ndcg = dcg(top_k)\n",
    "    indcg = dcg(ideal_top_k)\n",
    "    return ndcg/indcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd493fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,ys_train = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9668ce66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chana\\AppData\\Local\\Temp\\ipykernel_24080\\867369768.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(ys_train[q].reshape(50),dtype = torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.4288\n",
      "Epoch: 1, Loss: 0.4346\n",
      "Epoch: 1, Loss: 1.1254\n",
      "Epoch: 1, Loss: 0.2305\n",
      "Epoch: 1, Loss: 0.3009\n",
      "Epoch: 1, Loss: 0.3652\n",
      "Epoch: 1, Loss: 0.2998\n",
      "Epoch: 1, Loss: 0.2421\n",
      "Epoch: 1, Loss: 0.5149\n",
      "Epoch: 1, Loss: 0.3993\n",
      "Epoch: 1, Loss: 0.3688\n",
      "Epoch: 1, Loss: 0.3813\n",
      "Epoch: 1, Loss: 0.3102\n",
      "Epoch: 1, Loss: 0.2089\n",
      "Epoch: 1, Loss: 0.3035\n",
      "Epoch: 1, Loss: 0.4956\n",
      "Epoch: 1, Loss: 0.3158\n",
      "Epoch: 1, Loss: 0.3562\n",
      "Epoch: 1, Loss: 0.5776\n",
      "Epoch: 1, Loss: 0.3948\n",
      "Epoch: 1, Loss: 0.2637\n",
      "Epoch: 1, Loss: 0.1746\n",
      "Epoch: 1, Loss: 0.2860\n",
      "Epoch: 1, Loss: 0.3851\n",
      "Epoch: 1, Loss: 0.2539\n",
      "Epoch: 1, Loss: 0.2975\n",
      "Epoch: 1, Loss: 0.2415\n",
      "Epoch: 1, Loss: 0.3761\n",
      "Epoch: 1, Loss: 0.3335\n",
      "Epoch: 1, Loss: 0.6082\n",
      "Epoch: 1, Loss: 0.1871\n",
      "Epoch: 1, Loss: 0.8572\n",
      "Epoch: 1, Loss: 0.6419\n",
      "Epoch: 1, Loss: 0.1619\n",
      "Epoch: 1, Loss: 0.4678\n",
      "Epoch: 1, Loss: 0.2719\n",
      "Epoch: 1, Loss: 0.1566\n",
      "Epoch: 1, Loss: 0.1466\n",
      "Epoch: 1, Loss: 0.2197\n",
      "Epoch: 1, Loss: 0.3855\n",
      "Epoch: 1, Loss: 0.2217\n",
      "Epoch: 1, Loss: 0.3209\n",
      "Epoch: 1, Loss: 0.2209\n",
      "Epoch: 1, Loss: 0.8315\n",
      "Epoch: 1, Loss: 0.6471\n",
      "Epoch: 1, Loss: 0.3319\n",
      "Epoch: 1, Loss: 0.3750\n",
      "Epoch: 1, Loss: 0.6963\n",
      "Epoch: 1, Loss: 0.1395\n",
      "Epoch: 1, Loss: 0.1420\n",
      "Epoch: 1, Loss: 0.3782\n",
      "Epoch: 1, Loss: 0.0541\n",
      "Epoch: 1, Loss: 0.3788\n",
      "Epoch: 1, Loss: 0.0945\n",
      "Epoch: 1, Loss: 0.3508\n",
      "Epoch: 1, Loss: 0.2525\n",
      "Epoch: 1, Loss: 0.0992\n",
      "Epoch: 1, Loss: 0.1423\n",
      "Epoch: 1, Loss: 0.8424\n",
      "Epoch: 1, Loss: 0.4714\n",
      "Epoch: 1, Loss: 0.2593\n",
      "Epoch: 1, Loss: 0.8620\n",
      "Epoch: 1, Loss: 0.3209\n",
      "Epoch: 1, Loss: 0.7833\n",
      "Epoch: 1, Loss: 0.2945\n",
      "Epoch: 1, Loss: 0.0872\n",
      "Epoch: 1, Loss: 0.5525\n",
      "Epoch: 1, Loss: 0.2656\n",
      "Epoch: 1, Loss: 0.2336\n",
      "Epoch: 1, Loss: 0.2873\n",
      "Epoch: 1, Loss: 0.1490\n",
      "Epoch: 1, Loss: 0.0913\n",
      "Epoch: 1, Loss: 0.2498\n",
      "Epoch: 1, Loss: 0.5433\n",
      "Epoch: 1, Loss: 0.3767\n",
      "Epoch: 1, Loss: 0.1538\n",
      "Epoch: 1, Loss: 0.2366\n",
      "Epoch: 1, Loss: 0.1859\n",
      "Epoch: 1, Loss: 0.2328\n",
      "Epoch: 1, Loss: 0.2782\n",
      "Epoch: 1, Loss: 0.1267\n",
      "Epoch: 1, Loss: 0.3279\n",
      "Epoch: 1, Loss: 0.1586\n",
      "Epoch: 1, Loss: 0.4214\n",
      "Epoch: 1, Loss: 0.2113\n",
      "Epoch: 1, Loss: 0.5448\n",
      "Epoch: 1, Loss: 0.1127\n",
      "Epoch: 1, Loss: 0.5711\n",
      "Epoch: 1, Loss: 0.5901\n",
      "Epoch: 1, Loss: 0.1873\n",
      "Epoch: 1, Loss: 0.5378\n",
      "Epoch: 1, Loss: 0.2603\n",
      "Epoch: 1, Loss: 0.3488\n",
      "Epoch: 1, Loss: 0.4015\n",
      "Epoch: 1, Loss: 0.2987\n",
      "Epoch: 1, Loss: 0.4375\n",
      "Epoch: 1, Loss: 0.3692\n",
      "Epoch: 1, Loss: 0.2931\n",
      "Epoch: 1, Loss: 0.1980\n",
      "Epoch: 1, Loss: 0.2376\n",
      "Epoch: 1, Loss: 0.0727\n",
      "Epoch: 1, Loss: 0.3067\n",
      "Epoch: 1, Loss: 0.6194\n",
      "Epoch: 1, Loss: 0.3207\n",
      "Epoch: 1, Loss: 0.4113\n",
      "Epoch: 1, Loss: 0.1596\n",
      "Epoch: 1, Loss: 0.6277\n",
      "Epoch: 1, Loss: 0.2117\n",
      "Epoch: 1, Loss: 0.6867\n",
      "Epoch: 1, Loss: 0.9387\n",
      "Epoch: 1, Loss: 0.1835\n",
      "Epoch: 1, Loss: 0.7428\n",
      "Epoch: 1, Loss: 0.2141\n",
      "Epoch: 1, Loss: 1.0509\n",
      "Epoch: 1, Loss: 0.2620\n",
      "Epoch: 1, Loss: 0.2795\n",
      "Epoch: 1, Loss: 0.3371\n",
      "Epoch: 1, Loss: 0.2352\n",
      "Epoch: 1, Loss: 0.3497\n",
      "Epoch: 1, Loss: 0.2410\n",
      "Epoch: 1, Loss: 0.3181\n",
      "Epoch: 1, Loss: 0.2888\n",
      "Epoch: 1, Loss: 0.9651\n",
      "Epoch: 1, Loss: 0.2617\n",
      "Epoch: 1, Loss: 0.3833\n",
      "Epoch: 1, Loss: 0.3714\n",
      "Epoch: 1, Loss: 0.2312\n",
      "Epoch: 1, Loss: 0.3975\n",
      "Epoch: 1, Loss: 0.3395\n",
      "Epoch: 1, Loss: 0.2393\n",
      "Epoch: 1, Loss: 0.5279\n",
      "Epoch: 1, Loss: 0.3456\n",
      "Epoch: 1, Loss: 0.3400\n",
      "Epoch: 1, Loss: 0.4483\n",
      "Epoch: 1, Loss: 0.2980\n",
      "Epoch: 1, Loss: 0.3252\n",
      "Epoch: 1, Loss: 0.0435\n",
      "Epoch: 1, Loss: 0.3391\n",
      "Epoch: 1, Loss: 0.3429\n",
      "Epoch: 1, Loss: 0.0906\n",
      "Epoch: 1, Loss: 0.2550\n",
      "Epoch: 1, Loss: 0.2553\n",
      "Epoch: 1, Loss: 0.8168\n",
      "Epoch: 1, Loss: 0.3145\n",
      "Epoch: 1, Loss: 0.2464\n",
      "Epoch: 1, Loss: 0.3840\n",
      "Epoch: 1, Loss: 0.2413\n",
      "Epoch: 1, Loss: 0.2756\n",
      "Epoch: 1, Loss: 0.0453\n",
      "Epoch: 1, Loss: 0.3181\n",
      "Epoch: 1, Loss: 0.0373\n",
      "Epoch: 1, Loss: 0.1968\n",
      "Epoch: 1, Loss: 0.5315\n",
      "Epoch: 1, Loss: 0.2487\n",
      "Epoch: 1, Loss: 0.1033\n",
      "Epoch: 1, Loss: 0.2478\n",
      "Epoch: 1, Loss: 0.7436\n",
      "Epoch: 1, Loss: 0.2948\n",
      "Epoch: 1, Loss: 0.1938\n",
      "Epoch: 1, Loss: 0.4365\n",
      "Epoch: 1, Loss: 0.3372\n",
      "Epoch: 1, Loss: 0.4861\n",
      "Epoch: 1, Loss: 0.0145\n",
      "Epoch: 1, Loss: 0.2314\n",
      "Epoch: 1, Loss: 0.2856\n",
      "Epoch: 1, Loss: 0.1842\n",
      "Epoch: 1, Loss: 0.4461\n",
      "Epoch: 1, Loss: 0.1884\n",
      "Epoch: 1, Loss: 0.3237\n",
      "Epoch: 1, Loss: 0.3362\n",
      "Epoch: 1, Loss: 0.1740\n",
      "Epoch: 1, Loss: 0.2721\n",
      "Epoch: 1, Loss: 0.2559\n",
      "Epoch: 1, Loss: 0.4326\n",
      "Epoch: 1, Loss: 0.2750\n",
      "Epoch: 1, Loss: 0.0892\n",
      "Epoch: 1, Loss: 0.4138\n",
      "Epoch: 1, Loss: 0.3846\n",
      "Epoch: 1, Loss: 0.4398\n",
      "Epoch: 1, Loss: 0.2894\n",
      "Epoch: 1, Loss: 0.2147\n",
      "Epoch: 1, Loss: 0.2365\n",
      "Epoch: 1, Loss: 0.3150\n",
      "Epoch: 1, Loss: 0.4145\n",
      "Epoch: 1, Loss: 0.4868\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ListNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim2, output_dim):\n",
    "        super(ListNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def KL_div(p, q):\n",
    "    return (p * torch.log(p / q)).sum()\n",
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "input_dim = 136\n",
    "hidden_dim = 512\n",
    "hidden_dim2 = 256\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = ListNet(input_dim, hidden_dim,hidden_dim2, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train model\n",
    "for q in range(X_train.shape[0]):\n",
    "    X = X_train[q]\n",
    "    Y = torch.tensor(ys_train[q].reshape(50),dtype = torch.float64)\n",
    "    for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        output = nn.functional.softmax(output, dim=0)\n",
    "        Y = Y.reshape(50,1)\n",
    "        target = nn.functional.softmax(Y,dim=0)\n",
    "        loss = KL_div(output, target).float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {}, Loss: {:.4f}\".format(epoch+1, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da816ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chana\\AppData\\Local\\Temp\\ipykernel_24080\\2468370219.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return ndcg/indcg\n"
     ]
    }
   ],
   "source": [
    "n = X_train.shape[0]\n",
    "ndcg_list = []\n",
    "max_ndcg = 0\n",
    "for i in range(n):\n",
    "    output = model(X_train[i])\n",
    "    output = nn.functional.softmax(output,dim = 0)\n",
    "    output = np.array(output.detach().numpy())\n",
    "    output = output.reshape(50)\n",
    "\n",
    "    Y = np.array(ys_train[i].reshape(50))\n",
    "    \n",
    "    rank_pred = np.argsort(output)[::-1]\n",
    "    rank_score = Y[rank_pred]\n",
    "    ndcg = ndcg_k(rank_score, 10)\n",
    "    ndcg_list.append(ndcg)\n",
    "    if ndcg > max_ndcg:\n",
    "        max_ndcg = ndcg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ad9da45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4655806602111526"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ec25bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 181\n",
    "output = model(X_train[idx])\n",
    "output = nn.functional.softmax(output,dim = 0)\n",
    "output = np.array(output.detach().numpy())\n",
    "output = output.reshape(50)\n",
    "\n",
    "Y = np.array(ys_train[idx].reshape(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0f81a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_pred = np.argsort(output)[::-1]\n",
    "rank_score = Y[rank_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ea76b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9740816258588203"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_k(rank_score,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20f791b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, ys_val = clean_data(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c51fa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "5\n",
      "10\n",
      "16\n",
      "23\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "n = X_val.shape[0]\n",
    "ndcg_list = []\n",
    "max_ndcg = 0\n",
    "for i in range(n):\n",
    "    output = model(X_val[i])\n",
    "    output = nn.functional.softmax(output,dim = 0)\n",
    "    output = np.array(output.detach().numpy())\n",
    "    output = output.reshape(50)\n",
    "\n",
    "    Y = np.array(ys_val[i].reshape(50))\n",
    "    \n",
    "    rank_pred = np.argsort(output)[::-1]\n",
    "    rank_score = Y[rank_pred]\n",
    "    ndcg_val = ndcg_k(rank_score, 10)\n",
    "    ndcg_list.append(ndcg_val)\n",
    "    if ndcg_val > max_ndcg:\n",
    "        max_ndcg = ndcg_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0908a474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4316787742719746"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e58063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
